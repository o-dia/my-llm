{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fa3391",
   "metadata": {},
   "source": [
    "# Understanding Transformer Architecture\n",
    "\n",
    "This notebook provides a hands-on exploration of transformer architecture - the foundation of modern LLMs like GPT, Llama, and others. Through visualization and code exploration, we'll build an intuition for why transformers have revolutionized NLP.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The key components of transformer architecture\n",
    "- How self-attention works and why it's revolutionary\n",
    "- How positional encoding enables sequence processing\n",
    "- The role of multi-head attention\n",
    "- How to inspect and visualize these components in real models\n",
    "- The differences between encoder-only, decoder-only, and encoder-decoder architectures\n",
    "\n",
    "Let's start by setting up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050533c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (2.2.3)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (2.2.5)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages/huggingface_hub-0.29.2-py3.8.egg (from transformers) (0.29.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.39.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/my-llm/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading plotly-6.1.0-py3-none-any.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.39.1-py3-none-any.whl (355 kB)\n",
      "Installing collected packages: narwhals, plotly, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [seaborn]m2/3\u001b[0m [seaborn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed narwhals-1.39.1 plotly-6.1.0 seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "%pip install transformers torch matplotlib pandas seaborn numpy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set styles for better visualization\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Ensure plots appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable interactive widgets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc7ba3",
   "metadata": {},
   "source": [
    "## 1. The Evolution to Transformers\n",
    "\n",
    "Before diving into transformers, let's understand why they were such a breakthrough:\n",
    "\n",
    "1. **RNNs/LSTMs (Pre-2017)**: \n",
    "   - Processed text sequentially (one word at a time)\n",
    "   - Suffered from vanishing gradients with long sequences\n",
    "   - Limited parallel processing capabilities\n",
    "\n",
    "2. **Convolutional Networks for NLP**:\n",
    "   - Better parallelization than RNNs\n",
    "   - Limited receptive field (context window)\n",
    "\n",
    "3. **Transformers (2017+)**:\n",
    "   - Process entire sequences simultaneously\n",
    "   - Capture long-range dependencies through attention\n",
    "   - Highly parallelizable training\n",
    "\n",
    "The key innovation of transformers was the **attention mechanism**, which allowed each word to directly \"attend\" to all other words in a sequence, regardless of their positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b628a",
   "metadata": {},
   "source": [
    "## 2. Exploring Pre-trained Transformer Models\n",
    "\n",
    "Let's start by loading some pre-trained transformer models and examining their architecture. We'll look at a few different types:\n",
    "\n",
    "1. **GPT-2**: A decoder-only transformer (for text generation)\n",
    "2. **BERT**: An encoder-only transformer (for understanding text)\n",
    "3. **T5**: An encoder-decoder transformer (for translation/summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore a model's configuration and properties\n",
    "def explore_model(model_name):\n",
    "    \"\"\"\n",
    "    Load and explore a pre-trained model's configuration and architecture.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the HuggingFace model to load\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The loaded model and its configuration\n",
    "    \"\"\"\n",
    "    print(f\"Exploring model: {model_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Load model configuration first to examine parameters\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    print(f\"Model type: {config.model_type}\")\n",
    "    print(f\"Model architecture: {config.architectures if hasattr(config, 'architectures') else 'Not specified'}\")\n",
    "    \n",
    "    # Print key model dimensions\n",
    "    if hasattr(config, \"hidden_size\"):\n",
    "        print(f\"Hidden size: {config.hidden_size}\")\n",
    "    if hasattr(config, \"num_hidden_layers\"):\n",
    "        print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "    if hasattr(config, \"num_attention_heads\"):\n",
    "        print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "    if hasattr(config, \"intermediate_size\"):\n",
    "        print(f\"Intermediate (feed-forward) size: {config.intermediate_size}\")\n",
    "    \n",
    "    # Print model's vocabulary size\n",
    "    if hasattr(config, \"vocab_size\"):\n",
    "        print(f\"Vocabulary size: {config.vocab_size}\")\n",
    "    \n",
    "    # Calculate theoretical parameter count from config\n",
    "    try:\n",
    "        if config.model_type == \"gpt2\":\n",
    "            # Rough parameter calculation for GPT-2 style models\n",
    "            embed_params = config.vocab_size * config.hidden_size\n",
    "            pos_embed_params = config.max_position_embeddings * config.hidden_size\n",
    "            layer_params = 12 * config.hidden_size * config.hidden_size + \\\n",
    "                           4 * config.hidden_size * config.intermediate_size + \\\n",
    "                           config.intermediate_size * config.hidden_size + \\\n",
    "                           2 * config.hidden_size + config.intermediate_size\n",
    "            total_layer_params = layer_params * config.num_hidden_layers\n",
    "            approx_total = embed_params + pos_embed_params + total_layer_params\n",
    "            print(f\"Approximate parameter count from config: {approx_total:,}\")\n",
    "        else:\n",
    "            print(\"Parameter calculation from config not implemented for this model type\")\n",
    "    except:\n",
    "        print(\"Could not estimate parameters from config\")\n",
    "    \n",
    "    # Load the actual model\n",
    "    print(\"\\nLoading model weights to count actual parameters...\")\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Count actual parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Actual parameter count: {total_params:,}\")\n",
    "    \n",
    "    # Print trainable vs non-trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "    \n",
    "    print(\"\\nModel architecture summary:\")\n",
    "    # Print the first level of modules\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"  {name}: {module.__class__.__name__}\")\n",
    "        \n",
    "        # For the first layer, print more details\n",
    "        if \"encoder\" in name or \"decoder\" in name or name == \"transformer\":\n",
    "            # Try to find the first layer\n",
    "            first_layer = None\n",
    "            if hasattr(module, \"layer\") and hasattr(module.layer, \"0\"):\n",
    "                first_layer = module.layer[0]\n",
    "            elif hasattr(module, \"h\") and hasattr(module.h, \"0\"):\n",
    "                first_layer = module.h[0]\n",
    "            elif hasattr(module, \"layers\") and len(module.layers) > 0:\n",
    "                first_layer = module.layers[0]\n",
    "                \n",
    "            if first_layer is not None:\n",
    "                print(f\"\\nExamining the first layer:\")\n",
    "                for sub_name, sub_module in first_layer.named_children():\n",
    "                    print(f\"    {sub_name}: {sub_module.__class__.__name__}\")\n",
    "                    # For attention module, print even more details\n",
    "                    if \"attn\" in sub_name or \"attention\" in sub_name:\n",
    "                        for attn_name, attn_module in sub_module.named_children():\n",
    "                            print(f\"      {attn_name}: {attn_module.__class__.__name__}\")\n",
    "    \n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a decoder-only transformer (GPT-2)\n",
    "gpt2_model, gpt2_config = explore_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore an encoder-only transformer (BERT)\n",
    "bert_model, bert_config = explore_model(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore an encoder-decoder transformer (T5)\n",
    "t5_model, t5_config = explore_model(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d043f033",
   "metadata": {},
   "source": [
    "## 3. Understanding Self-Attention\n",
    "\n",
    "The heart of the transformer is the **self-attention mechanism**. This is what allows transformers to weigh the importance of words in relation to each other.\n",
    "\n",
    "Let's visualize and understand how self-attention works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize self-attention\n",
    "def visualize_self_attention(text, model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Tokenize a text, run it through a model, and visualize the self-attention patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        model_name (str): Model to use for analysis\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the token IDs and convert them back to tokens for display\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "    \n",
    "    # Run through the model and get attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get attention weights (shape: [layers, heads, seq_len, seq_len])\n",
    "    attention = outputs.attentions\n",
    "    \n",
    "    # Print model information\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Number of layers: {len(attention)}\")\n",
    "    print(f\"Number of attention heads: {attention[0].shape[1]}\")\n",
    "    print(f\"Input sequence length: {len(tokens)}\")\n",
    "    \n",
    "    # Select a specific layer and head to visualize\n",
    "    layer_idx = 0  # First layer\n",
    "    head_idx = 0   # First attention head\n",
    "    \n",
    "    # Create attention heatmap for the selected layer and head\n",
    "    attention_weights = attention[layer_idx][0, head_idx].numpy()\n",
    "    \n",
    "    # Create a DataFrame for the heatmap\n",
    "    df = pd.DataFrame(attention_weights, index=tokens, columns=tokens)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "    plt.title(f\"Self-Attention Weights (Layer {layer_idx+1}, Head {head_idx+1})\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Let's also create an interactive plot with Plotly\n",
    "    fig = px.imshow(\n",
    "        attention_weights,\n",
    "        x=tokens,\n",
    "        y=tokens,\n",
    "        color_continuous_scale='Viridis',\n",
    "        labels=dict(x=\"Key Tokens\", y=\"Query Tokens\", color=\"Attention Weight\"),\n",
    "        title=f\"Self-Attention Weights (Layer {layer_idx+1}, Head {head_idx+1})\"\n",
    "    )\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        width=700,\n",
    "        height=600,\n",
    "        xaxis=dict(side=\"top\"),\n",
    "    )\n",
    "    \n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "    \n",
    "    # Create a function to explore different layers and heads\n",
    "    def explore_attention(layer=0, head=0):\n",
    "        attention_weights = attention[layer][0, head].numpy()\n",
    "        df = pd.DataFrame(attention_weights, index=tokens, columns=tokens)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "        plt.title(f\"Self-Attention Weights (Layer {layer+1}, Head {head+1})\")\n",
    "        plt.ylabel(\"Query Tokens\")\n",
    "        plt.xlabel(\"Key Tokens\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Return the explore function for further exploration\n",
    "    return explore_attention, attention, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2055412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize self-attention patterns for a simple sentence\n",
    "explore_fn, attention_weights, tokens = visualize_self_attention(\n",
    "    \"The transformer architecture revolutionized natural language processing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at different layers and heads\n",
    "explore_fn(layer=5, head=3)  # Middle layer, different head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f258ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at the last layer\n",
    "explore_fn(layer=11, head=0)  # Last layer (for GPT-2), first head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90cb195",
   "metadata": {},
   "source": [
    "### Understanding Self-Attention Computation\n",
    "\n",
    "Now let's break down the mathematical computation behind self-attention. Self-attention involves three main steps:\n",
    "\n",
    "1. **Computing Query, Key, and Value vectors** from input embeddings\n",
    "2. **Calculating attention scores** between all tokens\n",
    "3. **Creating weighted representations** by aggregating values according to attention scores\n",
    "\n",
    "Let's implement these steps manually to understand the process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement self-attention from scratch\n",
    "def self_attention_from_scratch(input_embeddings, d_k=64):\n",
    "    \"\"\"\n",
    "    Implement vanilla self-attention from scratch.\n",
    "    \n",
    "    Args:\n",
    "        input_embeddings: Input token embeddings [batch_size, seq_len, embedding_dim]\n",
    "        d_k: Dimensionality of query and key vectors\n",
    "    \n",
    "    Returns:\n",
    "        context_vectors: Attention-weighted outputs\n",
    "        attention_weights: Attention weight matrix\n",
    "    \"\"\"\n",
    "    # For simplicity, we'll use random weight matrices\n",
    "    batch_size, seq_len, d_model = input_embeddings.shape\n",
    "    \n",
    "    # 1. Create random weight matrices for Q, K, V projections\n",
    "    W_Q = torch.randn(d_model, d_k)\n",
    "    W_K = torch.randn(d_model, d_k)\n",
    "    W_V = torch.randn(d_model, d_model)  # V typically projects to the same dimension\n",
    "    \n",
    "    # 2. Create Query, Key, Value projections\n",
    "    Q = torch.matmul(input_embeddings, W_Q)  # [batch_size, seq_len, d_k]\n",
    "    K = torch.matmul(input_embeddings, W_K)  # [batch_size, seq_len, d_k]\n",
    "    V = torch.matmul(input_embeddings, W_V)  # [batch_size, seq_len, d_model]\n",
    "    \n",
    "    # 3. Calculate attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch_size, seq_len, seq_len]\n",
    "    \n",
    "    # 4. Scale the scores (to prevent softmax saturation)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # 5. Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "    \n",
    "    # 6. Apply attention weights to values\n",
    "    context_vectors = torch.matmul(attention_weights, V)  # [batch_size, seq_len, d_model]\n",
    "    \n",
    "    return context_vectors, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some toy token embeddings and visualize self-attention\n",
    "def visualize_self_attention_computation():\n",
    "    # Create toy embeddings for a 5-token sequence\n",
    "    seq_len = 5\n",
    "    d_model = 64\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Random embeddings for demonstration\n",
    "    embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Run our self-attention implementation\n",
    "    context_vectors, attention_weights = self_attention_from_scratch(embeddings)\n",
    "    \n",
    "    # Visualize the attention weights\n",
    "    attention_matrix = attention_weights[0].detach().numpy()  # Remove batch dimension\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attention_matrix, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "    plt.title(\"Self-Attention Weights (From Scratch Implementation)\")\n",
    "    plt.xlabel(\"Key Positions\")\n",
    "    plt.ylabel(\"Query Positions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the query-key-value computation\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add steps for the self-attention computation\n",
    "    steps = [\n",
    "        \"Token Embeddings [seq_len, d_model]\",\n",
    "        \"Query Projection (Q) [seq_len, d_k]\",\n",
    "        \"Key Projection (K) [seq_len, d_k]\",\n",
    "        \"Value Projection (V) [seq_len, d_model]\",\n",
    "        \"Attention Scores = Q·K^T [seq_len, seq_len]\",\n",
    "        \"Scaled Attention = Scores/√d_k\",\n",
    "        \"Attention Weights = softmax(Scaled Attention)\",\n",
    "        \"Output = Weights·V [seq_len, d_model]\"\n",
    "    ]\n",
    "    \n",
    "    # Create a visualization of the self-attention process\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        y=[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        mode=\"markers+text\",\n",
    "        marker=dict(size=15, color=\"blue\"),\n",
    "        text=steps,\n",
    "        textposition=\"top center\"\n",
    "    ))\n",
    "    \n",
    "    # Add arrows to show the flow\n",
    "    for i in range(len(steps)-1):\n",
    "        fig.add_annotation(\n",
    "            x=i+1, y=1,\n",
    "            ax=i+2, ay=1,\n",
    "            xref=\"x\", yref=\"y\",\n",
    "            axref=\"x\", ayref=\"y\",\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            arrowsize=1.5,\n",
    "            arrowwidth=2,\n",
    "            arrowcolor=\"red\"\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Self-Attention Computation Flow\",\n",
    "        xaxis=dict(showticklabels=False, showgrid=False, zeroline=False),\n",
    "        yaxis=dict(showticklabels=False, showgrid=False, zeroline=False),\n",
    "        showlegend=False,\n",
    "        width=900,\n",
    "        height=300,\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0438617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualization\n",
    "attention_weights = visualize_self_attention_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3d4ef",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding: How Transformers Understand Order\n",
    "\n",
    "Unlike RNNs, transformers process all tokens simultaneously, losing the natural order of the sequence. **Positional encoding** solves this by adding position information to token embeddings.\n",
    "\n",
    "Let's visualize how positional encodings work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742876dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and visualize positional encoding\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Compute sinusoidal positional encodings as used in the original transformer paper.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Maximum sequence length\n",
    "        d_model: Dimensionality of the model embeddings\n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding: Positional encodings [seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # Create empty positional encoding matrix\n",
    "    pos_encoding = torch.zeros(seq_len, d_model)\n",
    "    \n",
    "    # Create position indices\n",
    "    positions = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Calculate frequencies for sine/cosine functions\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    \n",
    "    # Apply sine to even indices\n",
    "    pos_encoding[:, 0::2] = torch.sin(positions * div_term)\n",
    "    \n",
    "    # Apply cosine to odd indices\n",
    "    pos_encoding[:, 1::2] = torch.cos(positions * div_term)\n",
    "    \n",
    "    return pos_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings\n",
    "def visualize_positional_encoding():\n",
    "    # Generate positional encodings\n",
    "    seq_len = 30\n",
    "    d_model = 64\n",
    "    pos_enc = positional_encoding(seq_len, d_model)\n",
    "    \n",
    "    # Plot as a heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pos_enc, cmap=\"coolwarm\")\n",
    "    plt.title(\"Positional Encodings\")\n",
    "    plt.xlabel(\"Embedding Dimension\")\n",
    "    plt.ylabel(\"Position in Sequence\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot a few dimensions to show the wavelength patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot several dimensions\n",
    "    dims_to_plot = [0, 1, 2, 3, 15, 31, 47, 63]\n",
    "    for i, dim in enumerate(dims_to_plot):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.plot(pos_enc[:, dim])\n",
    "        plt.title(f\"Dimension {dim}\")\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Explain why this works\n",
    "    print(\"Why Sinusoidal Positional Encoding Works:\")\n",
    "    print(\"1. Each position has a unique encoding pattern\")\n",
    "    print(\"2. Relative positions have similar patterns at different frequencies\")\n",
    "    print(\"3. The model can learn to attend to similar positions through these patterns\")\n",
    "    print(\"4. Lower dimensions (higher frequencies) help distinguish nearby positions\")\n",
    "    print(\"5. Higher dimensions (lower frequencies) capture long-range patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26254365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the visualization\n",
    "visualize_positional_encoding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc807b4",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention: Looking from Different Perspectives\n",
    "\n",
    "Instead of a single attention mechanism, transformers use **multi-head attention**. Each head can focus on different aspects of the relationships between tokens.\n",
    "\n",
    "Let's visualize how different attention heads focus on different patterns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and visualize multi-head attention\n",
    "def visualize_multi_head_attention(text, model_name=\"gpt2\", layer_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize how different attention heads focus on different patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        model_name (str): Model to use for analysis\n",
    "        layer_idx (int): Which layer to analyze\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the token IDs and convert them back to tokens for display\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "    \n",
    "    # Run through the model and get attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get attention weights for the selected layer\n",
    "    attention = outputs.attentions[layer_idx][0]  # [heads, seq_len, seq_len]\n",
    "    \n",
    "    # Get number of heads\n",
    "    num_heads = attention.shape[0]\n",
    "    \n",
    "    # Create a figure for all attention heads\n",
    "    fig, axes = plt.subplots(2, (num_heads+1)//2, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each attention head\n",
    "    for head_idx in range(num_heads):\n",
    "        attention_weights = attention[head_idx].numpy()\n",
    "        sns.heatmap(attention_weights, annot=False, cmap=\"YlGnBu\", \n",
    "                   ax=axes[head_idx], xticklabels=False, yticklabels=False)\n",
    "        axes[head_idx].set_title(f\"Head {head_idx+1}\")\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_heads, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Multi-Head Attention Patterns (Layer {layer_idx+1})\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    plt.show()\n",
    "    \n",
    "    # Now create a more detailed visualization of a few selected heads\n",
    "    heads_to_show = min(4, num_heads)\n",
    "    \n",
    "    fig, axes = plt.subplots(heads_to_show, 1, figsize=(10, 12))\n",
    "    if heads_to_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(heads_to_show):\n",
    "        head_idx = i\n",
    "        attention_weights = attention[head_idx].numpy()\n",
    "        \n",
    "        # Create a DataFrame for better labeling\n",
    "        df = pd.DataFrame(attention_weights, index=tokens, columns=tokens)\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", ax=axes[i])\n",
    "        axes[i].set_title(f\"Head {head_idx+1} Attention Pattern\")\n",
    "        axes[i].set_ylabel(\"Query Tokens\")\n",
    "        if i == heads_to_show - 1:\n",
    "            axes[i].set_xlabel(\"Key Tokens\")\n",
    "        else:\n",
    "            axes[i].set_xlabel(\"\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Detailed Multi-Head Attention (Layer {layer_idx+1})\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze patterns - let's examine what each head might be focusing on\n",
    "    print(\"Potential attention patterns:\")\n",
    "    \n",
    "    # Compute some metrics for each head\n",
    "    for head_idx in range(num_heads):\n",
    "        weights = attention[head_idx].numpy()\n",
    "        \n",
    "        # Calculate diagonal attention (self-attention)\n",
    "        diagonal_attn = np.mean(np.diag(weights))\n",
    "        \n",
    "        # Calculate local attention (attention to neighboring tokens)\n",
    "        local_attn = 0\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(max(0, i-1), min(len(tokens), i+2)):\n",
    "                if i != j:  # Exclude the diagonal\n",
    "                    local_attn += weights[i, j]\n",
    "        local_attn /= (len(tokens) * 2 - 2)  # Normalize\n",
    "        \n",
    "        # Check for attention to the first token (often special tokens)\n",
    "        first_token_attn = np.mean(weights[:, 0])\n",
    "        \n",
    "        print(f\"Head {head_idx+1}:\")\n",
    "        print(f\"  - Self-attention strength: {diagonal_attn:.3f}\")\n",
    "        print(f\"  - Local attention strength: {local_attn:.3f}\")\n",
    "        print(f\"  - First token attention: {first_token_attn:.3f}\")\n",
    "        \n",
    "        # Analyze potential patterns based on metrics\n",
    "        if diagonal_attn > 0.5:\n",
    "            print(\"  - This head seems to focus on the token itself (identity/content)\")\n",
    "        elif local_attn > 0.3:\n",
    "            print(\"  - This head seems to focus on local relationships (syntax/phrases)\")\n",
    "        elif first_token_attn > 0.3:\n",
    "            print(\"  - This head pays significant attention to the first token (global context)\")\n",
    "        else:\n",
    "            max_col = np.argmax(np.mean(weights, axis=0))\n",
    "            if max_col > 0:\n",
    "                print(f\"  - This head focuses on token '{tokens[max_col]}'\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return attention, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411171c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-head attention for a more complex sentence\n",
    "attention_weights, tokens = visualize_multi_head_attention(\n",
    "    \"The quick brown fox jumps over the lazy dog while the cat watches from a distance.\",\n",
    "    layer_idx=2  # Using a middle layer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cd335",
   "metadata": {},
   "source": [
    "## 6. The Feed-Forward Network: Processing Token Representations\n",
    "\n",
    "After attention, each token's representation goes through a **feed-forward network** (FFN). This is a simple two-layer neural network applied to each position separately.\n",
    "\n",
    "Let's examine the feed-forward network in transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the feed-forward network\n",
    "def examine_feed_forward_network(model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Explore the structure and role of feed-forward networks in transformers.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Model to examine\n",
    "    \"\"\"\n",
    "    # Load model configuration\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    # Print feed-forward network dimensions\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Hidden size (embedding dimension): {config.hidden_size}\")\n",
    "    \n",
    "    # The intermediate size is typically the FFN's expanded dimension\n",
    "    if hasattr(config, \"intermediate_size\"):\n",
    "        ffn_dim = config.intermediate_size\n",
    "    else:\n",
    "        # For some models like GPT-2, it's typically 4x the hidden size\n",
    "        ffn_dim = 4 * config.hidden_size\n",
    "    \n",
    "    print(f\"Feed-forward intermediate dimension: {ffn_dim}\")\n",
    "    print(f\"Expansion factor: {ffn_dim / config.hidden_size}x\")\n",
    "    \n",
    "    # Visualize the feed-forward network structure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Define the layers\n",
    "    layers = [\n",
    "        {\"name\": \"Input\", \"size\": config.hidden_size},\n",
    "        {\"name\": \"Linear 1\", \"size\": ffn_dim},\n",
    "        {\"name\": \"GELU\", \"size\": ffn_dim},\n",
    "        {\"name\": \"Linear 2\", \"size\": config.hidden_size},\n",
    "        {\"name\": \"Output\", \"size\": config.hidden_size}\n",
    "    ]\n",
    "    \n",
    "    # Draw the network\n",
    "    max_size = max(ffn_dim, config.hidden_size)\n",
    "    \n",
    "    # Draw nodes\n",
    "    for i, layer in enumerate(layers):\n",
    "        size_factor = layer[\"size\"] / max_size\n",
    "        width = 2 + 8 * size_factor\n",
    "        \n",
    "        plt.plot([i, i], [0, width], 'b-', linewidth=3)\n",
    "        plt.text(i, width/2, layer[\"name\"], ha='center', va='center',\n",
    "                rotation=90, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Add size label\n",
    "        plt.text(i, width + 0.5, f\"{layer['size']}\", ha='center')\n",
    "    \n",
    "    # Draw connections\n",
    "    for i in range(len(layers)-1):\n",
    "        from_size = layers[i][\"size\"] / max_size\n",
    "        to_size = layers[i+1][\"size\"] / max_size\n",
    "        \n",
    "        from_width = 2 + 8 * from_size\n",
    "        to_width = 2 + 8 * to_size\n",
    "        \n",
    "        # Draw multiple lines to show the connection width\n",
    "        num_lines = 10\n",
    "        for j in range(num_lines):\n",
    "            from_y = j * from_width / (num_lines-1)\n",
    "            to_y = j * to_width / (num_lines-1)\n",
    "            plt.plot([i, i+1], [from_y, to_y], 'k-', alpha=0.1)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(\"Feed-Forward Network Structure in Transformer\", fontsize=16)\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Width (proportional to dimension)\")\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Explain the role of FFN\n",
    "    print(\"\\nRole of the Feed-Forward Network:\")\n",
    "    print(\"1. Processes each token's representation independently\")\n",
    "    print(\"2. Expands the representation to a higher dimension, allowing more expressivity\")\n",
    "    print(\"3. Applies non-linear transformation (GELU/ReLU) to capture complex patterns\")\n",
    "    print(\"4. Projects back to the original dimension for residual connections\")\n",
    "    print(\"5. Acts as a position-wise fully connected layer\")\n",
    "    print(\"\\nA study by Google Research suggests feed-forward layers act as 'key-value memories'\")\n",
    "    print(\"that store knowledge gained during training, functioning like a large associative memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ff3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the feed-forward network\n",
    "examine_feed_forward_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e601261",
   "metadata": {},
   "source": [
    "## 7. Putting It All Together: The Complete Transformer Architecture\n",
    "\n",
    "Now let's understand how all these components fit together in the transformer architecture. We'll look at:\n",
    "1. **Encoder-only models** (like BERT)\n",
    "2. **Decoder-only models** (like GPT)\n",
    "3. **Encoder-decoder models** (like T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the transformer architecture\n",
    "def visualize_transformer_architecture():\n",
    "    \"\"\"\n",
    "    Create visualizations of the three main transformer architecture types.\n",
    "    \"\"\"\n",
    "    # Create a figure with three subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 8))\n",
    "    \n",
    "    # Colors for different components\n",
    "    colors = {\n",
    "        \"embedding\": \"lightblue\",\n",
    "        \"positional\": \"lightyellow\",\n",
    "        \"attention\": \"lightgreen\",\n",
    "        \"ffn\": \"lightpink\",\n",
    "        \"norm\": \"lightgray\",\n",
    "        \"output\": \"lightcoral\"\n",
    "    }\n",
    "    \n",
    "    # 1. Encoder-only (BERT-like) architecture\n",
    "    encoder_components = [\n",
    "        {\"name\": \"Token Embedding\", \"color\": colors[\"embedding\"]},\n",
    "        {\"name\": \"Position Embedding\", \"color\": colors[\"positional\"]},\n",
    "        {\"name\": \"Layer Norm\", \"color\": colors[\"norm\"]},\n",
    "        {\"name\": \"Self-Attention\", \"color\": colors[\"attention\"]},\n",
    "        {\"name\": \"Layer Norm\", \"color\": colors[\"norm\"]},\n",
    "        {\"name\": \"Feed-Forward\", \"color\": colors[\"ffn\"]},\n",
    "        {\"name\": \"Layer Norm\", \"color\": colors[\"norm\"]},\n",
    "        {\"name\": \"Output\", \"color\": colors[\"output\"]}\n",
    "    ]\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, len(encoder_components) + 1)\n",
    "    \n",
    "    # Draw blocks for encoder\n",
    "    for i, comp in enumerate(encoder_components):\n",
    "        y_pos = len(encoder_components) - i\n",
    "        rect = plt.Rectangle((0.2, y_pos - 0.4), 0.6, 0.8, facecolor=comp[\"color\"])\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.text(0.5, y_pos, comp[\"name\"], ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # Add arrows\n",
    "        if i < len(encoder_components) - 1:\n",
    "            ax1.arrow(0.5, y_pos - 0.5, 0, -0.5, head_width=0.05, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Draw residual connections\n",
    "    ax1.arrow(0.7, len(encoder_components) - 3.5, 0, -2, head_width=0.05, head_length=0.1, \n",
    "             fc='blue', ec='blue', linestyle='--')\n",
    "    ax1.arrow(0.7, len(encoder_components) - 5.5, 0, -1, head_width=0.05, head_length=0.1, \n",
    "             fc='blue', ec='blue', linestyle='--')\n",
    "    \n",
    "    ax1.set_title(\"Encoder-Only Architecture\\n(BERT, RoBERTa)\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # 2. Decoder-only (GPT-like) architecture\n",
    "    decoder_components = [\n",
    "        {\"name\": \"Token Embedding\", \"color\": colors[\"embedding\"]},\n",
    "        {\"name\": \"Position Embedding\", \"color\": colors[\"positional\"]},\n",
    "        {\"name\": \"Layer Norm\", \"color\": colors[\"norm\"]},\n",
    "        {\"name\": \"Masked Self-Attention\", \"color\": colors[\"attention\"]},\n",
    "        {\"name\": \"Layer Norm\", \"color\": colors[\"norm\"]},\n",
    "        {\"name\": \"Feed-Forward\", \"color\": colors[\"ffn\"]},\n",
    "        {\"name\": \"Layer Norm\", \"color\": colors[\"norm\"]},\n",
    "        {\"name\": \"Output\", \"color\": colors[\"output\"]}\n",
    "    ]\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, len(decoder_components) + 1)\n",
    "    \n",
    "    # Draw blocks for decoder\n",
    "    for i, comp in enumerate(decoder_components):\n",
    "        y_pos = len(decoder_components) - i\n",
    "        rect = plt.Rectangle((0.2, y_pos - 0.4), 0.6, 0.8, facecolor=comp[\"color\"])\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(0.5, y_pos, comp[\"name\"], ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # Add arrows\n",
    "        if i < len(decoder_components) - 1:\n",
    "            ax2.arrow(0.5, y_pos - 0.5, 0, -0.5, head_width=0.05, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Draw residual connections\n",
    "    ax2.arrow(0.7, len(decoder_components) - 3.5, 0, -2, head_width=0.05, head_length=0.1, \n",
    "             fc='blue', ec='blue', linestyle='--')\n",
    "    ax2.arrow(0.7, len(decoder_components) - 5.5, 0, -1, head_width=0.05, head_length=0.1, \n",
    "             fc='blue', ec='blue', linestyle='--')\n",
    "    \n",
    "    ax2.set_title(\"Decoder-Only Architecture\\n(GPT, Llama)\", fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # 3. Encoder-decoder (T5-like) architecture\n",
    "    encoder_decoder_components = [\n",
    "        {\"side\": \"encoder\", \"name\": \"Token Embedding\", \"color\": colors[\"embedding\"]},\n",
    "        {\"side\": \"encoder\", \"name\": \"Position Embedding\", \"color\": colors[\"positional\"]},\n",
    "        {\"side\": \"encoder\", \"name\": \"Self-Attention\", \"color\": colors[\"attention\"]},\n",
    "        {\"side\": \"encoder\", \"name\": \"Feed-Forward\", \"color\": colors[\"ffn\"]},\n",
    "        {\"side\": \"encoder\", \"name\": \"Output\", \"color\": colors[\"norm\"]},\n",
    "        {\"side\": \"decoder\", \"name\": \"Token Embedding\", \"color\": colors[\"embedding\"]},\n",
    "        {\"side\": \"decoder\", \"name\": \"Position Embedding\", \"color\": colors[\"positional\"]},\n",
    "        {\"side\": \"decoder\", \"name\": \"Masked Self-Attention\", \"color\": colors[\"attention\"]},\n",
    "        {\"side\": \"decoder\", \"name\": \"Cross-Attention\", \"color\": \"lightsalmon\"},\n",
    "        {\"side\": \"decoder\", \"name\": \"Feed-Forward\", \"color\": colors[\"ffn\"]},\n",
    "        {\"side\": \"decoder\", \"name\": \"Output\", \"color\": colors[\"output\"]}\n",
    "    ]\n",
    "    \n",
    "    ax3.set_xlim(0, 1)\n",
    "    ax3.set_ylim(0, len(encoder_decoder_components) + 1)\n",
    "    \n",
    "    # Track the last positions for each side\n",
    "    last_encoder_pos = None\n",
    "    last_decoder_pos = None\n",
    "    \n",
    "    # Draw blocks for encoder-decoder\n",
    "    encoder_count = 0\n",
    "    decoder_count = 0\n",
    "    \n",
    "    for i, comp in enumerate(encoder_decoder_components):\n",
    "        y_pos = len(encoder_decoder_components) - i\n",
    "        \n",
    "        if comp[\"side\"] == \"encoder\":\n",
    "            encoder_count += 1\n",
    "            x_start = 0.1\n",
    "            rect = plt.Rectangle((x_start, y_pos - 0.4), 0.3, 0.8, facecolor=comp[\"color\"])\n",
    "            ax3.add_patch(rect)\n",
    "            ax3.text(x_start + 0.15, y_pos, comp[\"name\"], ha='center', va='center', fontsize=9)\n",
    "            \n",
    "            # Track position for cross-attention connection\n",
    "            if comp[\"name\"] == \"Output\":\n",
    "                last_encoder_pos = (x_start + 0.15, y_pos)\n",
    "            \n",
    "            # Add arrows for encoder\n",
    "            if encoder_count > 1:\n",
    "                ax3.arrow(x_start + 0.15, y_pos + 0.5, 0, -0.5, head_width=0.05, head_length=0.1, \n",
    "                         fc='black', ec='black')\n",
    "        else:\n",
    "            decoder_count += 1\n",
    "            x_start = 0.6\n",
    "            rect = plt.Rectangle((x_start, y_pos - 0.4), 0.3, 0.8, facecolor=comp[\"color\"])\n",
    "            ax3.add_patch(rect)\n",
    "            ax3.text(x_start + 0.15, y_pos, comp[\"name\"], ha='center', va='center', fontsize=9)\n",
    "            \n",
    "            # Track position for cross-attention connection\n",
    "            if comp[\"name\"] == \"Cross-Attention\":\n",
    "                last_decoder_pos = (x_start + 0.15, y_pos)\n",
    "            \n",
    "            # Add arrows for decoder\n",
    "            if decoder_count > 1:\n",
    "                ax3.arrow(x_start + 0.15, y_pos + 0.5, 0, -0.5, head_width=0.05, head_length=0.1, \n",
    "                         fc='black', ec='black')\n",
    "    \n",
    "    # Draw the cross-attention connection\n",
    "    if last_encoder_pos and last_decoder_pos:\n",
    "        ax3.arrow(last_encoder_pos[0], last_encoder_pos[1], \n",
    "                 last_decoder_pos[0] - last_encoder_pos[0], \n",
    "                 last_decoder_pos[1] - last_encoder_pos[1], \n",
    "                 head_width=0.05, head_length=0.1, fc='red', ec='red', linestyle='-.')\n",
    "    \n",
    "    ax3.set_title(\"Encoder-Decoder Architecture\\n(T5, BART)\", fontsize=14)\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Three Main Transformer Architecture Types\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create table comparing the three architectures\n",
    "    architectures = [\"Encoder-Only\", \"Decoder-Only\", \"Encoder-Decoder\"]\n",
    "    features = [\n",
    "        \"Primary Use Cases\", \n",
    "        \"Context Processing\", \n",
    "        \"Attention Type\", \n",
    "        \"Input/Output\", \n",
    "        \"Training Objective\", \n",
    "        \"Example Models\"\n",
    "    ]\n",
    "    \n",
    "    data = [\n",
    "        [\"Understanding tasks (classification, NER, etc.)\", \"Text generation\", \"Translation, summarization, QA\"],\n",
    "        [\"Bidirectional (sees full context)\", \"Unidirectional (only previous tokens)\", \"Full context in encoder, unidirectional in decoder\"],\n",
    "        [\"Full self-attention\", \"Masked self-attention\", \"Both + cross-attention\"],\n",
    "        [\"Fixed-length inputs\", \"Variable length outputs\", \"Input → different output\"],\n",
    "        [\"Masked language modeling\", \"Next token prediction\", \"Sequence-to-sequence\"],\n",
    "        [\"BERT, RoBERTa, DeBERTa\", \"GPT, Llama, Falcon\", \"T5, BART, Pegasus\"]\n",
    "    ]\n",
    "    \n",
    "    # Create table\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=data, rowLabels=features, colLabels=architectures,\n",
    "                    loc='center', cellLoc='center', colWidths=[0.3, 0.3, 0.3])\n",
    "    \n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    # Color the header row\n",
    "    for j, cell in enumerate(table._cells[(0, j)] for j in range(3)):\n",
    "        cell.set_facecolor('#4472C4')\n",
    "        cell.set_text_props(color='white')\n",
    "    \n",
    "    # Color the row labels\n",
    "    for i, cell in enumerate(table._cells[(i, -1)] for i in range(1, 7)):\n",
    "        cell.set_facecolor('#8EA9DB')\n",
    "    \n",
    "    plt.title(\"Comparison of Transformer Architectures\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complete transformer architecture\n",
    "visualize_transformer_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857c655",
   "metadata": {},
   "source": [
    "## 8. Exercises: Hands-On Transformer Exploration\n",
    "\n",
    "Now that we've explored the transformer architecture, let's consolidate our learning with some hands-on exercises:\n",
    "\n",
    "### Exercise 1: Compare Attention Patterns Across Different Models\n",
    "\n",
    "Choose a sentence and compare how different models (e.g., GPT-2, BERT, T5) attend to it. What differences do you notice?\n",
    "\n",
    "### Exercise 2: Visualize Attention for Specific Language Phenomena\n",
    "\n",
    "Try inputs with specific language phenomena (e.g., coreference resolution, subject-verb agreement) and examine how attention heads capture these relationships.\n",
    "\n",
    "### Exercise 3: Modify and Observe Self-Attention\n",
    "\n",
    "Implement a custom self-attention layer with modifications (e.g., different scaling, adding a bias) and observe how it changes the attention patterns.\n",
    "\n",
    "### Exercise 4: Examine Model Scaling Properties\n",
    "\n",
    "Look at how model parameters scale with different sizes of models (e.g., GPT-2 small vs. medium). Plot the relationship between model size and parameter count.\n",
    "\n",
    "Let's start with Exercise 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compare attention patterns across different models\n",
    "def compare_model_attention(text):\n",
    "    \"\"\"\n",
    "    Compare attention patterns across different model types.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "    \"\"\"\n",
    "    models = [\"gpt2\", \"bert-base-uncased\", \"t5-small\"]\n",
    "    model_types = [\"Decoder-only (GPT-2)\", \"Encoder-only (BERT)\", \"Encoder-decoder (T5)\"]\n",
    "    \n",
    "    # Create a figure for comparison\n",
    "    fig, axes = plt.subplots(len(models), 1, figsize=(12, 6*len(models)))\n",
    "    \n",
    "    for i, (model_name, model_type) in enumerate(zip(models, model_types)):\n",
    "        print(f\"Processing {model_type}...\")\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # For encoder models, use AutoModel\n",
    "        # For decoder models, use AutoModelForCausalLM\n",
    "        # This handles different model types correctly\n",
    "        if \"t5\" in model_name:\n",
    "            model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "        elif \"gpt\" in model_name:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "        else:\n",
    "            model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get tokens for labeling\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "        tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
    "        \n",
    "        # Run through the model and get attention weights\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get attention weights from last layer, first head\n",
    "        # (This will work for most models, but might need adjustment for some)\n",
    "        attentions = outputs.attentions\n",
    "        \n",
    "        if attentions is not None:\n",
    "            # Get the last layer's attention (usually most informative)\n",
    "            last_layer_attention = attentions[-1]\n",
    "            \n",
    "            # Get attention from first head (for simplicity)\n",
    "            attention_weights = last_layer_attention[0, 0].numpy()\n",
    "            \n",
    "            # Create a DataFrame for the heatmap\n",
    "            df = pd.DataFrame(attention_weights, index=tokens, columns=tokens)\n",
    "            \n",
    "            # Plot the heatmap\n",
    "            sns.heatmap(df, annot=False, cmap=\"YlGnBu\", ax=axes[i])\n",
    "            axes[i].set_title(f\"{model_type} Attention Pattern (Last Layer, First Head)\")\n",
    "            axes[i].set_ylabel(\"Query Tokens\")\n",
    "            \n",
    "            if i == len(models) - 1:\n",
    "                axes[i].set_xlabel(\"Key Tokens\")\n",
    "            \n",
    "            # Print model-specific notes\n",
    "            print(f\"  Number of layers: {len(attentions)}\")\n",
    "            print(f\"  Number of attention heads: {last_layer_attention.shape[1]}\")\n",
    "            print(f\"  Sequence length: {len(tokens)}\")\n",
    "            print(f\"  Tokens: {tokens}\")\n",
    "            print()\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, \"Attention weights not available for this model\", \n",
    "                        ha='center', va='center')\n",
    "            axes[i].set_title(f\"{model_type}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Observations:\")\n",
    "    print(\"1. GPT-2 (decoder) uses causal masking - each token only attends to previous tokens\")\n",
    "    print(\"2. BERT (encoder) has full bidirectional attention - each token can attend to all tokens\")\n",
    "    print(\"3. T5 (encoder-decoder) has different attention patterns in encoder and decoder\")\n",
    "    print(\"4. Different models tokenize text differently, affecting the attention patterns\")\n",
    "    print(\"5. Special tokens like [CLS], [SEP], <s>, </s> often receive or distribute a lot of attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfedeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Exercise 1\n",
    "compare_model_attention(\"The transformer architecture revolutionized natural language processing by enabling parallel computation and better handling of long-range dependencies.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fcd7c6",
   "metadata": {},
   "source": [
    "## 9. Additional Exercises\n",
    "\n",
    "Continue with the remaining exercises on your own. Here are some starting points:\n",
    "\n",
    "### Exercise 2: Visualize Attention for Specific Language Phenomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c194b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try inputs with coreference resolution\n",
    "explore_fn_coref, _, _ = visualize_self_attention(\n",
    "    \"John said he was tired. Mary thought he needed rest.\"\n",
    ")\n",
    "\n",
    "# Look at different layers and heads to see if any capture the coreference\n",
    "explore_fn_coref(layer=5, head=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f24e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try inputs with subject-verb agreement\n",
    "explore_fn_agreement, _, _ = visualize_self_attention(\n",
    "    \"The keys to the cabinet are on the table.\"\n",
    ")\n",
    "\n",
    "# Look at different layers and heads\n",
    "explore_fn_agreement(layer=8, head=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa01368",
   "metadata": {},
   "source": [
    "### Exercise 3: Modify and Observe Self-Attention\n",
    "\n",
    "Let's modify our self-attention implementation with different scaling approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05517002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify self-attention with different scaling approaches\n",
    "def modified_self_attention(input_embeddings, d_k=64, scaling_factor=None, add_bias=False):\n",
    "    \"\"\"\n",
    "    Implement modified self-attention with optional scaling and bias.\n",
    "    \n",
    "    Args:\n",
    "        input_embeddings: Input token embeddings [batch_size, seq_len, embedding_dim]\n",
    "        d_k: Dimensionality of query and key vectors\n",
    "        scaling_factor: Custom scaling factor (None for no scaling)\n",
    "        add_bias: Whether to add a bias term to attention scores\n",
    "    \n",
    "    Returns:\n",
    "        context_vectors: Attention-weighted outputs\n",
    "        attention_weights: Attention weight matrix\n",
    "    \"\"\"\n",
    "    # For simplicity, we'll use random weight matrices\n",
    "    batch_size, seq_len, d_model = input_embeddings.shape\n",
    "    \n",
    "    # Create random weight matrices for Q, K, V projections\n",
    "    W_Q = torch.randn(d_model, d_k)\n",
    "    W_K = torch.randn(d_model, d_k)\n",
    "    W_V = torch.randn(d_model, d_model)\n",
    "    \n",
    "    # Create Query, Key, Value projections\n",
    "    Q = torch.matmul(input_embeddings, W_Q)\n",
    "    K = torch.matmul(input_embeddings, W_K)\n",
    "    V = torch.matmul(input_embeddings, W_V)\n",
    "    \n",
    "    # Calculate attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # Apply custom scaling if provided\n",
    "    if scaling_factor is not None:\n",
    "        scores = scores / scaling_factor\n",
    "    \n",
    "    # Add bias if requested\n",
    "    if add_bias:\n",
    "        # Create a bias that encourages local attention\n",
    "        bias = torch.zeros_like(scores)\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                bias[0, i, j] = -0.1 * abs(i - j)  # Penalize distant tokens\n",
    "        scores = scores + bias\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply attention weights to values\n",
    "    context_vectors = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return context_vectors, attention_weights\n",
    "\n",
    "# Function to compare different attention variants\n",
    "def compare_attention_variants():\n",
    "    # Create toy embeddings for demonstration\n",
    "    seq_len = 5\n",
    "    d_model = 64\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Same random embeddings for fair comparison\n",
    "    torch.manual_seed(42)\n",
    "    embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Test different variants\n",
    "    variants = [\n",
    "        (\"Standard (scaled by √d_k)\", lambda e: self_attention_from_scratch(e)),\n",
    "        (\"No scaling\", lambda e: modified_self_attention(e, scaling_factor=None)),\n",
    "        (\"Aggressive scaling (÷10)\", lambda e: modified_self_attention(e, scaling_factor=10)),\n",
    "        (\"With local bias\", lambda e: modified_self_attention(e, add_bias=True))\n",
    "    ]\n",
    "    \n",
    "    # Create a figure for comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Run each variant and visualize\n",
    "    for i, (name, func) in enumerate(variants):\n",
    "        _, attention_weights = func(embeddings)\n",
    "        \n",
    "        # Visualize the attention weights\n",
    "        attention_matrix = attention_weights[0].detach().numpy()\n",
    "        \n",
    "        # Create a heatmap\n",
    "        sns.heatmap(attention_matrix, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", ax=axes[i])\n",
    "        axes[i].set_title(f\"Self-Attention Variant: {name}\")\n",
    "        axes[i].set_xlabel(\"Key Positions\")\n",
    "        axes[i].set_ylabel(\"Query Positions\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze the differences\n",
    "    print(\"Observations about attention variants:\")\n",
    "    print(\"1. Standard scaling (÷√d_k) balances the attention distribution\")\n",
    "    print(\"2. No scaling leads to more extreme softmax values (near 0 or 1)\")\n",
    "    print(\"3. Aggressive scaling makes attention more uniform\")\n",
    "    print(\"4. Local bias encourages attention to nearby tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436be973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different attention variants\n",
    "compare_attention_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4ffc9",
   "metadata": {},
   "source": [
    "### Exercise 4: Examine Model Scaling Properties\n",
    "\n",
    "Let's examine how model parameters scale with different model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine model scaling properties\n",
    "def examine_model_scaling():\n",
    "    \"\"\"\n",
    "    Compare parameter counts across different model sizes and visualize scaling relationships.\n",
    "    \"\"\"\n",
    "    # List of models with increasing sizes\n",
    "    models = [\n",
    "        \"distilgpt2\",           # ~82M parameters\n",
    "        \"gpt2\",                 # ~124M parameters\n",
    "        \"gpt2-medium\",          # ~355M parameters\n",
    "        \"gpt2-large\",           # ~774M parameters\n",
    "        \"gpt2-xl\"               # ~1.5B parameters\n",
    "    ]\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    hidden_sizes = []\n",
    "    layer_counts = []\n",
    "    head_counts = []\n",
    "    param_counts = []\n",
    "    \n",
    "    print(\"Analyzing model scaling properties...\")\n",
    "    \n",
    "    # Collect data for each model\n",
    "    for model_name in models:\n",
    "        try:\n",
    "            print(f\"Loading configuration for {model_name}...\")\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            \n",
    "            # Get model dimensions\n",
    "            hidden_size = config.hidden_size\n",
    "            n_layers = config.num_hidden_layers\n",
    "            n_heads = config.num_attention_heads\n",
    "            \n",
    "            # Calculate parameter count (approximate formula for GPT-2 style models)\n",
    "            vocab_size = config.vocab_size\n",
    "            embed_params = vocab_size * hidden_size\n",
    "            pos_embed_params = config.max_position_embeddings * hidden_size\n",
    "            \n",
    "            # Per-layer parameters\n",
    "            layer_params = (\n",
    "                # Self-attention\n",
    "                4 * hidden_size * hidden_size +  # Q, K, V, and output projections\n",
    "                # Feed-forward\n",
    "                4 * hidden_size * hidden_size * 4 +  # Expansion and projection (4x hidden size)\n",
    "                # Layer norms\n",
    "                4 * hidden_size  # 2 layer norms with gain and bias\n",
    "            )\n",
    "            \n",
    "            total_params = embed_params + pos_embed_params + (layer_params * n_layers)\n",
    "            \n",
    "            # Store the data\n",
    "            hidden_sizes.append(hidden_size)\n",
    "            layer_counts.append(n_layers)\n",
    "            head_counts.append(n_heads)\n",
    "            param_counts.append(total_params)\n",
    "            \n",
    "            print(f\"  Hidden size: {hidden_size}\")\n",
    "            print(f\"  Layers: {n_layers}\")\n",
    "            print(f\"  Attention heads: {n_heads}\")\n",
    "            print(f\"  Parameter count (approx): {total_params:,}\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_name}: {e}\")\n",
    "    \n",
    "    # Convert to more readable units (millions of parameters)\n",
    "    param_counts_m = [p / 1_000_000 for p in param_counts]\n",
    "    \n",
    "    # Plot the relationships\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Hidden size vs. Parameters\n",
    "    axes[0, 0].plot(hidden_sizes, param_counts_m, 'o-', linewidth=2, markersize=10)\n",
    "    axes[0, 0].set_title(\"Model Size vs. Parameter Count\")\n",
    "    axes[0, 0].set_xlabel(\"Hidden Size\")\n",
    "    axes[0, 0].set_ylabel(\"Parameters (millions)\")\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # 2. Layer count vs. Parameters\n",
    "    axes[0, 1].plot(layer_counts, param_counts_m, 'o-', linewidth=2, markersize=10)\n",
    "    axes[0, 1].set_title(\"Layer Count vs. Parameter Count\")\n",
    "    axes[0, 1].set_xlabel(\"Number of Layers\")\n",
    "    axes[0, 1].set_ylabel(\"Parameters (millions)\")\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # 3. Head count vs. Parameters\n",
    "    axes[1, 0].plot(head_counts, param_counts_m, 'o-', linewidth=2, markersize=10)\n",
    "    axes[1, 0].set_title(\"Attention Head Count vs. Parameter Count\")\n",
    "    axes[1, 0].set_xlabel(\"Number of Attention Heads\")\n",
    "    axes[1, 0].set_ylabel(\"Parameters (millions)\")\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # 4. Scatter plot of hidden size vs layers with size indicating parameters\n",
    "    scatter = axes[1, 1].scatter(hidden_sizes, layer_counts, s=[p/5_000_000 for p in param_counts], \n",
    "                              alpha=0.7, c=param_counts_m, cmap='viridis')\n",
    "    axes[1, 1].set_title(\"Model Architecture Comparison\")\n",
    "    axes[1, 1].set_xlabel(\"Hidden Size\")\n",
    "    axes[1, 1].set_ylabel(\"Number of Layers\")\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # Add colorbar for parameter count\n",
    "    cbar = fig.colorbar(scatter, ax=axes[1, 1])\n",
    "    cbar.set_label('Parameters (millions)')\n",
    "    \n",
    "    # Add model names as annotations\n",
    "    for i, model_name in enumerate(models):\n",
    "        axes[1, 1].annotate(model_name, (hidden_sizes[i], layer_counts[i]),\n",
    "                         xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print scaling observations\n",
    "    print(\"Observations about model scaling:\")\n",
    "    print(\"1. Parameter count scales quadratically with hidden size (d_model)\")\n",
    "    print(\"2. Parameter count scales linearly with the number of layers\")\n",
    "    print(\"3. Most parameters are in attention and feed-forward layers\")\n",
    "    print(\"4. Doubling the hidden size approximately quadruples the parameter count\")\n",
    "    print(\"5. Larger models can use fewer heads per dimension of hidden size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247222be",
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_model_scaling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7b5d0",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "After completing this notebook, you should understand:\n",
    "\n",
    "1. **The core components of transformers**:\n",
    "   - Self-attention mechanisms for capturing relationships between tokens\n",
    "   - Positional encoding for sequence order awareness\n",
    "   - Multi-head attention for capturing different types of relationships\n",
    "   - Feed-forward networks for processing token representations\n",
    "\n",
    "2. **Different transformer architectures**:\n",
    "   - Encoder-only (BERT): Best for understanding tasks\n",
    "   - Decoder-only (GPT): Best for generation tasks\n",
    "   - Encoder-decoder (T5): Best for translation/transformation tasks\n",
    "\n",
    "3. **How attention works**:\n",
    "   - Computes relationships between all pairs of tokens\n",
    "   - Uses query, key, and value projections\n",
    "   - Different heads can focus on different linguistic patterns\n",
    "\n",
    "4. **Why transformers revolutionized NLP**:\n",
    "   - Parallelizable (no sequential processing like RNNs)\n",
    "   - Capable of handling long-range dependencies\n",
    "   - Highly scalable architecture\n",
    "\n",
    "5. **Model scaling properties**:\n",
    "   - Parameter count grows quadratically with model width\n",
    "   - Deeper models capture more complex relationships\n",
    "\n",
    "In the next notebook, we'll explore how transformers are trained, fine-tuned, and optimized for specific tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
