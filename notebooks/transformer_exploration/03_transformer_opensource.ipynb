{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-Source LLMs: Models, Ecosystem, and Application Development\n",
    "\n",
    "This notebook explores the rich ecosystem of open-source large language models (LLMs), how to evaluate and use them effectively, and how to build applications with them. Building on your understanding of transformer architecture and fine-tuning techniques, we'll now focus on the practical aspects of leveraging these models to create valuable products.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- The current landscape of open-source LLMs and their capabilities\n",
    "- How to evaluate and select the right model for different use cases\n",
    "- Practical deployment considerations and optimization techniques\n",
    "- How to build applications with open-source LLMs using popular frameworks\n",
    "- Product development considerations when working with LLMs\n",
    "- How to implement evaluation frameworks to measure model performance\n",
    "\n",
    "Let's start by setting up our environment with the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers datasets accelerate huggingface_hub sentence-transformers langchain langchain-community \\\n",
    "    llama-cpp-python llamaindex chainlit torch optimum tiktoken einops auto-gptq bitsandbytes peft faiss-cpu \\\n",
    "    evaluate openai gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "# Set plotting styles\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up HuggingFace API (will need a token if accessing gated models)\n",
    "hf_api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Open-Source LLM Ecosystem\n",
    "\n",
    "The landscape of open-source LLMs has exploded in recent years, offering increasingly powerful models that rival commercial alternatives. Understanding this ecosystem is essential for making informed decisions about which models to use for your applications.\n",
    "\n",
    "### Key Open-Source Model Families\n",
    "\n",
    "Let's explore the major open-source LLM families and their characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of major open-source LLM families\n",
    "model_families = {\n",
    "    \"Family\": [\n",
    "        \"Llama\", \n",
    "        \"Mistral\", \n",
    "        \"Phi\", \n",
    "        \"Gemma\", \n",
    "        \"Pythia\", \n",
    "        \"Falcon\", \n",
    "        \"BLOOM\", \n",
    "        \"Qwen\", \n",
    "        \"MPT\",\n",
    "        \"Orca\",\n",
    "        \"Yi\",\n",
    "        \"OLMo\"\n",
    "    ],\n",
    "    \"Creator\": [\n",
    "        \"Meta\", \n",
    "        \"Mistral AI\", \n",
    "        \"Microsoft\", \n",
    "        \"Google\", \n",
    "        \"EleutherAI\", \n",
    "        \"TII\", \n",
    "        \"BigScience\", \n",
    "        \"Alibaba\", \n",
    "        \"MosaicML\",\n",
    "        \"Microsoft\",\n",
    "        \"01.AI\",\n",
    "        \"AI2\"\n",
    "    ],\n",
    "    \"Latest Version\": [\n",
    "        \"Llama-3 (70B)\", \n",
    "        \"Mistral 7B + Mixtral 8x7B\", \n",
    "        \"Phi-3 (mini/small/medium)\", \n",
    "        \"Gemma 2 (2B/7B/27B)\", \n",
    "        \"Pythia (1B-12B)\", \n",
    "        \"Falcon-180B\", \n",
    "        \"BLOOM-176B\", \n",
    "        \"Qwen2 (0.5B-72B)\", \n",
    "        \"MPT-30B\",\n",
    "        \"Orca 2 (13B)\",\n",
    "        \"Yi-34B\",\n",
    "        \"OLMo 7B\"\n",
    "    ],\n",
    "    \"Key Strengths\": [\n",
    "        \"Strong all-round performance, widely used\", \n",
    "        \"Efficient, strong reasoning, MoE architecture\", \n",
    "        \"Small but powerful, math capabilities\", \n",
    "        \"Lightweight with strong capabilities\", \n",
    "        \"Open weights, multiple checkpoints\", \n",
    "        \"Arabic language capabilities, large context\", \n",
    "        \"Multilingual (46+ languages)\", \n",
    "        \"Strong in Chinese and English, tool use\", \n",
    "        \"Long context window, ALiBi positioning\",\n",
    "        \"Instruction-following, reasoning\",\n",
    "        \"Quality in Chinese and English\",\n",
    "        \"Full transparency in training\"\n",
    "    ],\n",
    "    \"License\": [\n",
    "        \"Meta AI LLAMA 2, LLAMA 3 agreements\", \n",
    "        \"Apache 2.0\", \n",
    "        \"MIT\", \n",
    "        \"Apache 2.0\", \n",
    "        \"Apache 2.0\", \n",
    "        \"TII license\", \n",
    "        \"RAIL\", \n",
    "        \"Qwen License\", \n",
    "        \"Apache 2.0\",\n",
    "        \"MIT\",\n",
    "        \"Apache 2.0\",\n",
    "        \"Apache 2.0\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "model_df = pd.DataFrame(model_families)\n",
    "display(model_df.style.set_properties(**{'text-align': 'left'}).set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}]))\n",
    "\n",
    "# Create a more detailed visualization of model sizes\n",
    "def visualize_model_sizes():\n",
    "    # Model sizes in billions of parameters\n",
    "    models = [\n",
    "        \"Phi-3 Mini (3.8B)\", \"Gemma 2B\", \"Phi-3 Small (7B)\", \"Mistral 7B\", \"Llama-3 8B\", \n",
    "        \"Orca 2 (13B)\", \"Phi-3 Medium (14B)\", \"Gemma 2 (27B)\", \"Yi (34B)\", \"Llama-3 70B\", \n",
    "        \"Qwen2 72B\", \"Mixtral 8x7B (MoE)\", \"Falcon-180B\", \"BLOOM-176B\"\n",
    "    ]\n",
    "    sizes = [\n",
    "        3.8, 2, 7, 7, 8, \n",
    "        13, 14, 27, 34, 70, \n",
    "        72, 47, 180, 176  # Mixtral effective size is lower due to MoE\n",
    "    ]\n",
    "    \n",
    "    # Create categories\n",
    "    categories = [\n",
    "        \"Small\", \"Small\", \"Small\", \"Small\", \"Small\",\n",
    "        \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"Large\",\n",
    "        \"Large\", \"Large (MoE)\", \"Very Large\", \"Very Large\"\n",
    "    ]\n",
    "    \n",
    "    # Create color map\n",
    "    category_colors = {\"Small\": \"#1f77b4\", \"Medium\": \"#ff7f0e\", \"Large\": \"#2ca02c\", \"Large (MoE)\": \"#9467bd\", \"Very Large\": \"#d62728\"}\n",
    "    colors = [category_colors[cat] for cat in categories]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(models, sizes, color=colors)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Billions of Parameters (log scale)')\n",
    "    plt.title('Open-Source LLM Sizes')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add size labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        label_x_pos = width * 1.01\n",
    "        plt.text(label_x_pos, bar.get_y() + bar.get_height()/2, f'{width}B', \n",
    "                 va='center', fontsize=8)\n",
    "    \n",
    "    # Add a legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=category_colors[cat], label=cat) for cat in category_colors]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_model_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Innovations\n",
    "\n",
    "Open-source LLMs continue to evolve with new architectural innovations that improve performance, efficiency, and capabilities. Let's explore some of the key innovations in recent models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of key LLM innovations\n",
    "innovations = {\n",
    "    \"Innovation\": [\n",
    "        \"Mixture of Experts (MoE)\",\n",
    "        \"Grouped-Query Attention (GQA)\",\n",
    "        \"Sliding Window Attention\",\n",
    "        \"ALiBi Positional Encoding\",\n",
    "        \"RoPE (Rotary Position Embedding)\",\n",
    "        \"Multi-Query Attention (MQA)\",\n",
    "        \"Flash Attention\",\n",
    "        \"GELU Activation Function\"\n",
    "    ],\n",
    "    \"Description\": [\n",
    "        \"Routes tokens through specialized sub-networks, activating only a subset for each token\",\n",
    "        \"Shares key/value pairs across groups of attention heads for efficiency\",\n",
    "        \"Limits attention to a sliding window of tokens for efficiency with longer contexts\",\n",
    "        \"Attention with Linear Biases - adds bias to attention scores based on distance\",\n",
    "        \"Applies rotations to embedding vectors based on position, better preserves token relationships\",\n",
    "        \"Each query head attends to the same key/value pair for efficiency\",\n",
    "        \"Optimized attention algorithm that reduces memory usage and improves speed\",\n",
    "        \"Gaussian Error Linear Unit - smoother activation function than ReLU\"\n",
    "    ],\n",
    "    \"Benefits\": [\n",
    "        \"Larger effective model size with fewer active parameters, better specialization\",\n",
    "        \"Reduces memory usage and increases inference speed\",\n",
    "        \"Enables longer context processing with linear scaling\",\n",
    "        \"Better extrapolation to longer sequences than learned positional embeddings\",\n",
    "        \"Better handling of relative positions and sequence extrapolation\",\n",
    "        \"Reduces memory bandwidth requirements during inference\",\n",
    "        \"Significantly faster training and inference with less memory\",\n",
    "        \"Better gradient flow and performance than ReLU\"\n",
    "    ],\n",
    "    \"Example Models\": [\n",
    "        \"Mixtral 8x7B, Qwen1.5-MoE, DeepSeek-MoE\",\n",
    "        \"Llama 2/3, Gemma, Qwen\",\n",
    "        \"Mistral, Phi-2\",\n",
    "        \"MPT, Pythia, Dolly\",\n",
    "        \"Llama, Mistral, Falcon, Yi\",\n",
    "        \"PaLM, Gemma (variant)\",\n",
    "        \"Most recent models use this implementation\",\n",
    "        \"BERT, GPT-2/3/4, most modern LLMs\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "innovations_df = pd.DataFrame(innovations)\n",
    "display(innovations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of the most impactful innovations - Mixture of Experts (MoE) - to understand how it allows models to achieve greater effective capacity with fewer active parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Mixture of Experts (MoE) architecture\n",
    "def visualize_moe():\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Define the figure layout\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Draw the input\n",
    "    plt.text(50, 95, \"Input Token Representation\", ha='center', fontsize=14, fontweight='bold')\n",
    "    plt.arrow(50, 90, 0, -5, head_width=1, head_length=1, fc='black', ec='black')\n",
    "    \n",
    "    # Draw the router\n",
    "    router_rect = plt.Rectangle((40, 75), 20, 10, fc='#ff9999', ec='black')\n",
    "    ax.add_patch(router_rect)\n",
    "    plt.text(50, 80, \"Router Network\", ha='center', va='center', fontsize=11)\n",
    "    \n",
    "    # Draw the experts\n",
    "    expert_colors = ['#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6', '#c2f0c2', '#99ccff', '#ffb3b3']\n",
    "    expert_positions = [(10+i*20, 50) for i in range(8)]\n",
    "    expert_rects = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(expert_positions):\n",
    "        expert_rect = plt.Rectangle((x-7.5, y-10), 15, 20, fc=expert_colors[i], ec='black')\n",
    "        ax.add_patch(expert_rect)\n",
    "        plt.text(x, y, f\"Expert\\n{i+1}\", ha='center', va='center', fontsize=9)\n",
    "        expert_rects.append(expert_rect)\n",
    "    \n",
    "    # Draw connections from router to experts\n",
    "    active_experts = [1, 3, 6]  # Let's assume experts 1, 3, and 6 are active for this token\n",
    "    for i in range(8):\n",
    "        x, y = expert_positions[i]\n",
    "        alpha = 1.0 if i in active_experts else 0.2\n",
    "        width = 2.0 if i in active_experts else 0.5\n",
    "        linestyle = '-' if i in active_experts else '--'\n",
    "        plt.plot([50, x], [75, y+10], color='black', alpha=alpha, linewidth=width, linestyle=linestyle)\n",
    "    \n",
    "    # Draw the output combiner\n",
    "    plt.arrow(10, 40, 0, -5, head_width=1, head_length=1, fc='black', ec='black', alpha=0.2)\n",
    "    plt.arrow(30, 40, 0, -5, head_width=1, head_length=1, fc='black', ec='black')\n",
    "    plt.arrow(50, 40, 0, -5, head_width=1, head_length=1, fc='black', ec='black', alpha=0.2)\n",
    "    plt.arrow(70, 40, 0, -5, head_width=1, head_length=1, fc='black', ec='black')\n",
    "    plt.arrow(90, 40, 0, -5, head_width=1, head_length=1, fc='black', ec='black', alpha=0.2)\n",
    "    \n",
    "    combiner_rect = plt.Rectangle((40, 25), 20, 10, fc='#9999ff', ec='black')\n",
    "    ax.add_patch(combiner_rect)\n",
    "    plt.text(50, 30, \"Output Combiner\", ha='center', va='center', fontsize=11)\n",
    "    \n",
    "    # Draw the output\n",
    "    plt.arrow(50, 25, 0, -5, head_width=1, head_length=1, fc='black', ec='black')\n",
    "    plt.text(50, 15, \"Output Token Representation\", ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add title and description\n",
    "    plt.figtext(0.5, 0.02, \"In Mixture of Experts (MoE), only a subset of 'expert' networks process each token.\\n\"\n",
    "                \"This allows for much larger total parameter counts while keeping compute requirements manageable.\", \n",
    "                ha='center', fontsize=12, bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "    \n",
    "    plt.suptitle(\"Mixture of Experts (MoE) Architecture\", fontsize=16, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a comparison table between standard and MoE models\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Metric\": [\"Total Parameters\", \"Active Parameters per Token\", \"Computational Cost\", \"Memory Usage\", \"Specialization\", \"Examples\"],\n",
    "        \"Dense Model\": [\"70B\", \"70B (100%)\", \"Proportional to model size\", \"Full model size\", \"General-purpose\", \"Llama-3 70B, GPT-3.5\"],\n",
    "        \"MoE Model\": [\"47B (Effective 125B)\", \"13B (~10-30%)\", \"Much lower than equivalent dense model\", \"Can be higher due to expert parameters\", \"Experts specialize in different tasks\", \"Mixtral 8x7B, Qwen-MoE\"]\n",
    "    })\n",
    "    \n",
    "    display(comparison)\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_moe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licensing and Usage Restrictions\n",
    "\n",
    "Understanding licensing is crucial for product development. Open-source LLMs use various licenses, some with commercial restrictions. Let's explain the main license types and their implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison of common LLM licenses\n",
    "license_info = {\n",
    "    \"License\": [\n",
    "        \"Apache 2.0\",\n",
    "        \"MIT\",\n",
    "        \"Meta Llama 2/3 License\",\n",
    "        \"RAIL (Responsible AI License)\",\n",
    "        \"Custom Research-Only\"\n",
    "    ],\n",
    "    \"Commercial Use\": [\n",
    "        \"✅ Permitted\",\n",
    "        \"✅ Permitted\",\n",
    "        \"✅ Permitted (with limitations)\",\n",
    "        \"⚠️ Limited - requires responsible use\",\n",
    "        \"❌ Not permitted\"\n",
    "    ],\n",
    "    \"Modification\": [\n",
    "        \"✅ Permitted\",\n",
    "        \"✅ Permitted\",\n",
    "        \"✅ Permitted\",\n",
    "        \"✅ Permitted with same restrictions\",\n",
    "        \"⚠️ Limited to research\"\n",
    "    ],\n",
    "    \"Distribution\": [\n",
    "        \"✅ Permitted with attribution\",\n",
    "        \"✅ Permitted with attribution\",\n",
    "        \"⚠️ Additional terms for redistribution\",\n",
    "        \"⚠️ Responsible use requirements apply\",\n",
    "        \"⚠️ Typically only for academic purposes\"\n",
    "    ],\n",
    "    \"Use Cases Prohibited\": [\n",
    "        \"None specified\",\n",
    "        \"None specified\",\n",
    "        \"Various harmful uses, 700M+ MAU without approval\",\n",
    "        \"Deception, harassment, illegality, harm\",\n",
    "        \"Typically all commercial applications\"\n",
    "    ],\n",
    "    \"Example Models\": [\n",
    "        \"Mistral, Yi, Gemma, OLMo, Phi\",\n",
    "        \"Phi-3, Orca-2\",\n",
    "        \"Llama-2, Llama-3\",\n",
    "        \"BLOOM, FLAN-T5\",\n",
    "        \"Some early GPT-Neo variants\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "license_df = pd.DataFrame(license_info)\n",
    "display(license_df)\n",
    "\n",
    "# Highlight key considerations for product managers\n",
    "display(Markdown(\"\"\"### Key Licensing Considerations for Product Managers\n",
    "\n",
    "When building products with open-source LLMs, consider these license factors:\n",
    "\n",
    "1. **Content Generation Risk**: Even with permissive licenses, you're responsible for user-generated outputs\n",
    "2. **MAU Limitations**: Some licenses (like Llama) restrict usage beyond certain user thresholds\n",
    "3. **Attribution Requirements**: Most licenses require proper attribution in your products\n",
    "4. **Liability Issues**: Most licenses provide no warranty or liability protection\n",
    "5. **Combining Models**: When fine-tuning or merging models, licensing becomes more complex\n",
    "6. **Monitoring Changes**: License terms for popular models can evolve over time\n",
    "7. **Internal vs. Customer-Facing**: Some licenses have different terms based on how models are deployed\n",
    "\n",
    "For product development, Apache 2.0 and MIT licenses generally offer the most flexibility.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating and Selecting Open-Source LLMs\n",
    "\n",
    "Choosing the right model involves understanding the trade-offs between different models based on their capabilities, size, and computational requirements. Let's create a structured approach to model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a framework for model selection\n",
    "selection_framework = pd.DataFrame({\n",
    "    \"Consideration\": [\n",
    "        \"Task Requirements\",\n",
    "        \"Model Size\",\n",
    "        \"Compute Resources\",\n",
    "        \"Inference Speed\",\n",
    "        \"Specialization\",\n",
    "        \"Multilinguality\",\n",
    "        \"Licensing\",\n",
    "        \"Ecosystem Support\"\n",
    "    ],\n",
    "    \"Questions to Ask\": [\n",
    "        \"What specific capabilities does my application need? (reasoning, code generation, creativity, etc.)\",\n",
    "        \"What's the maximum model size my infrastructure can support? Consider RAM requirements.\",\n",
    "        \"Do you have access to GPUs/TPUs? What memory constraints exist?\",\n",
    "        \"What response time is acceptable for your application?\",\n",
    "        \"Is your use case better served by a specialized model or a general-purpose one?\",\n",
    "        \"What languages does your application need to support?\",\n",
    "        \"What licensing constraints apply to your business use case?\",\n",
    "        \"How well supported is the model in frameworks and tools you plan to use?\"\n",
    "    ],\n",
    "    \"Recommendations\": [\n",
    "        \"Map specific tasks to benchmark results; different models excel at different tasks\",\n",
    "        \"Smaller ≠ worse; consider Phi-3, Gemma 2, or Mistral 7B for efficiency\",\n",
    "        \"Consider 4-bit/8-bit quantization for larger models; MoE models when RAM is limited\",\n",
    "        \"Smaller models or optimized variants (GPTQ, GGUF) for lower latency\",\n",
    "        \"For code: CodeLlama, Starcoder; For math: Phi models; For reasoning: Mistral\",\n",
    "        \"BLOOM, XGLM for broad coverage; Yi, Qwen for Asian languages\",\n",
    "        \"Apache 2.0/MIT for maximum flexibility; check MAU limits and other restrictions\",\n",
    "        \"Llama, Mistral, and Phi families have the broadest adapter/library support\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(selection_framework)\n",
    "\n",
    "# Create a visualization of the LLM performance landscape\n",
    "def visualize_model_performance_landscape():\n",
    "    # Model performance data (approximated based on various benchmarks)\n",
    "    # This is simplified data for visualization purposes\n",
    "    models = [\n",
    "        \"Llama-3 70B\", \"Mixtral 8x7B\", \"Llama-3 8B\", \"Mistral 7B\", \n",
    "        \"Phi-3 Small\", \"Phi-3 Medium\", \"Gemma 2 27B\", \"Gemma 2 7B\", \"Gemma 2 2B\",\n",
    "        \"Yi 34B\", \"Qwen2 7B\", \"Qwen2 72B\"\n",
    "    ]\n",
    "    \n",
    "    reasoning = [92, 87, 80, 78, 76, 85, 86, 75, 65, 84, 77, 90]  # Reasoning capabilities\n",
    "    knowledge = [90, 85, 82, 75, 73, 83, 81, 73, 63, 80, 76, 89]  # Knowledge capabilities\n",
    "    sizes = [70, 47, 8, 7, 7, 14, 27, 7, 2, 34, 7, 72]  # Size in billions of parameters\n",
    "    \n",
    "    # Create a scatterplot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Size the points by model size\n",
    "    sizes_scaled = [100 * (s/max(sizes)) + 100 for s in sizes]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sc = plt.scatter(knowledge, reasoning, s=sizes_scaled, alpha=0.7, \n",
    "                    c=sizes, cmap='viridis', edgecolors='black', linewidths=1)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, model in enumerate(models):\n",
    "        plt.annotate(model, (knowledge[i], reasoning[i]), \n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Add a color bar to indicate model size\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.set_label('Model Size (B parameters)', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    plt.axhline(y=80, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=80, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    plt.text(65, 95, \"High Reasoning\\nLower Knowledge\", ha='center', fontsize=10)\n",
    "    plt.text(95, 95, \"Strong Generalists\\nHigh Reasoning & Knowledge\", ha='center', fontsize=10)\n",
    "    plt.text(65, 65, \"Smaller Models\\nMore Limited Capabilities\", ha='center', fontsize=10)\n",
    "    plt.text(95, 65, \"Strong Knowledge\\nLower Reasoning\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Knowledge Capabilities')\n",
    "    plt.ylabel('Reasoning Capabilities')\n",
    "    plt.title('LLM Performance Landscape (Approximate)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_model_performance_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Models for Your Specific Task\n",
    "\n",
    "While general benchmarks provide a high-level overview, it's crucial to evaluate models on tasks specific to your application. Let's create a simple framework for evaluating models on custom tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate models on specific tasks\n",
    "def evaluate_models_on_task(models, task_examples, evaluation_criteria, models_config=None):\n",
    "    \"\"\"\n",
    "    A framework for evaluating multiple models on a specific task.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names or paths\n",
    "        task_examples: List of input examples for the task\n",
    "        evaluation_criteria: Dictionary of criteria functions that score outputs\n",
    "        models_config: Optional configuration parameters for each model\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    results = {\"Model\": []}\n",
    "    for criterion in evaluation_criteria:\n",
    "        results[criterion] = []\n",
    "    results[\"Inference Time (s)\"] = []\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # For demonstration purposes, we'll simulate the evaluation\n",
    "    # In a real scenario, you would load each model and run inference\n",
    "    \n",
    "    # Simulated performance data (would be calculated from actual model outputs)\n",
    "    performance_data = {\n",
    "        \"llama-3-8b\": {\"Accuracy\": 0.82, \"Relevance\": 0.85, \"Creativity\": 0.78, \"Time\": 0.8},\n",
    "        \"mistral-7b-instruct\": {\"Accuracy\": 0.79, \"Relevance\": 0.88, \"Creativity\": 0.75, \"Time\": 0.7},\n",
    "        \"phi-3-mini\": {\"Accuracy\": 0.75, \"Relevance\": 0.80, \"Creativity\": 0.72, \"Time\": 0.5}\n",
    "    }\n",
    "    \n",
    "    for model in models:\n",
    "        model_id = model.split(\"/\")[-1].lower()\n",
    "        results[\"Model\"].append(model)\n",
    "        \n",
    "        # In a real implementation, you would:\n",
    "        # 1. Load the model\n",
    "        # 2. Run inference on each example\n",
    "        # 3. Apply evaluation criteria to outputs\n",
    "        # 4. Aggregate scores across examples\n",
    "        \n",
    "        # For this demonstration, we'll use simulated data\n",
    "        for criterion in evaluation_criteria:\n",
    "            if model_id in performance_data and criterion in performance_data[model_id]:\n",
    "                results[criterion].append(performance_data[model_id][criterion])\n",
    "            else:\n",
    "                # Randomly generate a score if no data available\n",
    "                import random\n",
    "                results[criterion].append(round(random.uniform(0.65, 0.90), 2))\n",
    "        \n",
    "        # Add inference time\n",
    "        if model_id in performance_data and \"Time\" in performance_data[model_id]:\n",
    "            results[\"Inference Time (s)\"].append(performance_data[model_id][\"Time\"])\n",
    "        else:\n",
    "            results[\"Inference Time (s)\"].append(round(random.uniform(0.5, 2.0), 1))\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add an overall score (weighted average)\n",
    "    weights = {\"Accuracy\": 0.5, \"Relevance\": 0.3, \"Creativity\": 0.2}\n",
    "    results_df[\"Overall Score\"] = sum(results_df[criterion] * weight for criterion, weight in weights.items())\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Define some sample task examples (e.g., creative writing prompts)\n",
    "task_examples = [\n",
    "    \"Write a short story about an AI assistant helping a product manager design a new app.\",\n",
    "    \"Create a marketing description for a smart home device that leverages AI.\",\n",
    "    \"Draft an email to stakeholders explaining the benefits of our new LLM-powered feature.\"\n",
    "]\n",
    "\n",
    "# Define evaluation criteria (would be functions that score outputs in a real implementation)\n",
    "evaluation_criteria = {\"Accuracy\": None, \"Relevance\": None, \"Creativity\": None}\n",
    "\n",
    "# Models to evaluate\n",
    "models_to_evaluate = [\n",
    "    \"meta-llama/Llama-3-8B-Instruct\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"microsoft/phi-3-mini\"\n",
    "]\n",
    "\n",
    "# Run the evaluation (simulated)\n",
    "results_df = evaluate_models_on_task(models_to_evaluate, task_examples, evaluation_criteria)\n",
    "\n",
    "# Display results\n",
    "display(results_df)\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_evaluation_results(results_df):\n",
    "    # Create a radar chart for each model\n",
    "    criteria = [c for c in results_df.columns if c not in [\"Model\", \"Inference Time (s)\", \"Overall Score\"]]\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(criteria)\n",
    "    \n",
    "    # Create angles for each criterion\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Add criterion labels\n",
    "    plt.xticks(angles[:-1], criteria, fontsize=12)\n",
    "    \n",
    "    # Draw y-axis labels (0.2 to 1.0 by 0.2)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot each model\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "        model_name = row[\"Model\"].split(\"/\")[-1]  # Extract just the model name for cleaner labels\n",
    "        values = [row[criterion] for criterion in criteria]\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot values\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=model_name, color=colors[i % len(colors)])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    \n",
    "    plt.title(\"Model Performance Comparison\", size=15, y=1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create a bar chart for overall scores and inference time\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot overall scores\n",
    "    models = [m.split(\"/\")[-1] for m in results_df[\"Model\"]]\n",
    "    ax1.bar(models, results_df[\"Overall Score\"], color=colors[:len(models)])\n",
    "    ax1.set_title(\"Overall Performance Score\")\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    for i, v in enumerate(results_df[\"Overall Score\"]):\n",
    "        ax1.text(i, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "        \n",
    "    # Plot inference times\n",
    "    ax2.bar(models, results_df[\"Inference Time (s)\"], color=colors[:len(models)])\n",
    "    ax2.set_title(\"Inference Time (lower is better)\")\n",
    "    ax2.set_ylabel(\"Seconds\")\n",
    "    for i, v in enumerate(results_df[\"Inference Time (s)\"]):\n",
    "        ax2.text(i, v + 0.05, f\"{v:.1f}s\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the evaluation results\n",
    "visualize_evaluation_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Optimization and Deployment\n",
    "\n",
    "Once you've selected a model, you'll need to optimize it for deployment. This includes quantization, pruning, and efficient serving techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison of model optimization techniques\n",
    "optimization_techniques = pd.DataFrame({\n",
    "    \"Technique\": [\n",
    "        \"Quantization (4-bit)\",\n",
    "        \"Quantization (8-bit)\",\n",
    "        \"GPTQ Quantization\",\n",
    "        \"GGUF Format\",\n",
    "        \"KV Cache Optimization\",\n",
    "        \"Pruning\",\n",
    "        \"Speculative Decoding\",\n",
    "        \"Flash Attention Implementation\",\n",
    "        \"Continuous Batching\"\n",
    "    ],\n",
    "    \"Description\": [\n",
    "        \"Reduce precision from 16/32-bit to 4-bit\",\n",
    "        \"Reduce precision from 16/32-bit to 8-bit\",\n",
    "        \"Quantization with additional optimizations\",\n",
    "        \"Optimized model format for CPU and GPU inference\",\n",
    "        \"Optimizing attention key/value caching\",\n",
    "        \"Removing less important weights\",\n",
    "        \"Using a smaller model to 'draft' predictions\",\n",
    "        \"Optimized implementation of attention mechanism\",\n",
    "        \"Process multiple requests together dynamically\"\n",
    "    ],\n",
    "    \"Memory Reduction\": [\n",
    "        \"~75-80%\",\n",
    "        \"~50%\",\n",
    "        \"~75%\",\n",
    "        \"~60-75%\",\n",
    "        \"15-30%\",\n",
    "        \"10-30%\",\n",
    "        \"N/A\",\n",
    "        \"30-40%\",\n",
    "        \"N/A\"\n",
    "    ],\n",
    "    \"Speed Improvement\": [\n",
    "        \"0.8-2x\",\n",
    "        \"1.2-1.5x\",\n",
    "        \"1.5-3x\",\n",
    "        \"1.5-4x\",\n",
    "        \"1.3-1.7x\",\n",
    "        \"1.1-1.3x\",\n",
    "        \"2-5x\",\n",
    "        \"2-4x\",\n",
    "        \"2-10x (for multiple requests)\"\n",
    "    ],\n",
    "    \"Quality Impact\": [\n",
    "        \"Moderate loss\",\n",
    "        \"Minimal loss\",\n",
    "        \"Low to moderate loss\",\n",
    "        \"Format-dependent\",\n",
    "        \"None\",\n",
    "        \"Low to moderate loss\",\n",
    "        \"None (with appropriate draft model)\",\n",
    "        \"None\",\n",
    "        \"None\"\n",
    "    ],\n",
    "    \"Implementation Complexity\": [\n",
    "        \"Low with bitsandbytes\",\n",
    "        \"Low\",\n",
    "        \"Medium\",\n",
    "        \"Low with llama.cpp\",\n",
    "        \"Medium\",\n",
    "        \"High\",\n",
    "        \"High\",\n",
    "        \"Low with recent libraries\",\n",
    "        \"High\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(optimization_techniques)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
